# Continuous Integration testing for KHARMA
# a.k.a did we break the basics?

# Build on Nvidia image.
# Can pretty easily change this out, with changes to build
# Someday we'll build & push a KHARMA image, then test that
image: nvcr.io/nvidia/nvhpc:23.5-devel-cuda12.1-rockylinux8

variables:
  OMP_NUM_THREADS: 8
  OMP_PROC_BIND: "false"
  MPI_EXE: mpirun
  MPI_NUM_PROCS: 2
  OMPI_ALLOW_RUN_AS_ROOT: 1
  OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: 1
  GIT_SUBMODULE_STRATEGY: recursive

### DEFAULT TEST BEHAVIOR ###
default:
  tags:
    - public-kharma-gpu
  # Be default: install pyharm, then run test in cwd
  # For new tests, write one run.sh script which runs/verifies
  # interleaved, and prints a summary of results.
  before_script:
    - export PATH="$HOME/.local/bin:$PATH"
    - wget -O Miniforge3.sh "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
    - bash Miniforge3.sh -b -p "/home/conda"
    - source "/home/conda/etc/profile.d/conda.sh"
    - conda install h5py
    - git clone https://github.com/AFD-Illinois/pyharm.git /home/pyharm
    - conda activate
    - cd /home/pyharm
    - pip install --user .
    - cd -

# Tests can be executed in parallel,
# but be careful about GPU arch
stages:
  - build
  - tests

# Build, obviously overrides script/artifacts
build:
  stage: build
  variables:
    NPROC: 8
    HOST_ARCH: NATIVE
  before_script:
    - echo "Skipping pyharm install in build."
  script:
    - export PREFIX_PATH=$PWD/external/hdf5
    - ./make.sh clean cuda hdf5
  artifacts:
    paths:
      - kharma.*
      - make_args

#Run all tests in parallel
tests:
  stage: tests
  script:
    - cd tests/$TEST
    - ./run.sh
  parallel:
    matrix:
      - TEST: [bondi, bondi_viscous, bz_monopole, emhdmodes, mhdmodes, noh, regrid, reinit, restart, tilt_init, torus_sanity]
