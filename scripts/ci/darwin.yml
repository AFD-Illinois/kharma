# Continuous Integration testing for KHARMA
# a.k.a did we break the basics?
# This version run on LANL Darwin
# See .gitlab-ci-docker.yml for a generic version,
# which can be run on any Docker runner w/GPUs

variables:
  GIT_SUBMODULE_STRATEGY: recursive
  SCHEDULER_PARAMETERS: "-N 1 --qos=debug -p volta-x86"
  HOST_ARCH: HSW
  NPROC: ""
  OMP_NUM_THREADS: 28
  OMP_PROC_BIND: "false"
  MPI_EXE: mpirun
  MPI_NUM_PROCS: 2
  HTTP_PROXY: http://proxyout.lanl.gov:8080
  http_proxy: http://proxyout.lanl.gov:8080
  HTTPS_PROXY: http://proxyout.lanl.gov:8080
  https_proxy: http://proxyout.lanl.gov:8080
  NO_PROXY: lanl.gov,localhost,127.0.0.1,0.0.0.0,::1
  no_proxy: lanl.gov,localhost,127.0.0.1,0.0.0.0,::1

### DEFAULT TEST BEHAVIOR ###
default:
  tags:
    - darwin-slurm-shared
  # Load Python
  before_script:
    - module load miniconda3

  # Always keep logs and plots.  Results should be printed to console!
  artifacts:
    when: always
    paths:
      - tests/*/*.png
      - tests/*/*.txt

# Tests can be executed in parallel,
# but be careful about GPU arch
stages:
  - build
  - tests

# Default rules
.default-rules:
  rules:
    - if: $CI_COMMIT_BRANCH == "dev"
      when: always
    - when: manual
  allow_failure: false

# Build, obviously overrides script/artifacts
build:
  extends: .default-rules
  stage: build
  before_script:
    - echo "Skipping pyharm install in build."
  script:
    - export PREFIX_PATH=$PWD/external/hdf5
    - ./make.sh clean cuda hdf5 volta ompi
  artifacts:
    paths:
      - kharma.*
      - make_args

# Run all tests in parallel
tests:
  extends: .default-rules
  stage: tests
  script:
    - cd tests/$TEST
    - ./run.sh
  parallel:
    matrix:
      - TEST: [bondi, bondi_viscous, bz_monopole, emhdmodes, mhdmodes, mhdmodes_smr,
               noh, regrid, reinit, restart, tilt_init, torus_sanity]

