#!/bin/bash

#COBALT -A NGM_EHT
#COBALT -t 600
# full-node. Could use single-gpu for really limited stuff
#COBALT -q full-node
#COBALT -n 8
N_NODES=8

# We specifically pass this on submit 
#cd $QSUB_DIR

KHARMA_DIR=~/kharma

echo "Starting Theta job script on " $(date)
echo "COBALT_NODEFILE: $COBALT_NODEFILE"
echo "COBALT_JOBID: $COBALT_JOBID"
#echo "Runs will use nodes:"
#mpirun -n $N_NODES -N 1 --hostfile $COBALT_NODEFILE hostname

DO_STRONG=false
DO_WEAK=true

KHARMA_DIR=~/kharma

# Use NVHPC bundled MPI on ThetaGPU since I guess nobody else compiles with net support?
#export CPATH="/soft/thetagpu/hpc-sdk/Linux_x86_64/21.3/comm_libs/mpi/include:$CPATH"
#export LD_LIBRARY_PATH="/soft/thetagpu/hpc-sdk/Linux_x86_64/21.3/comm_libs/mpi/lib:$LD_LIBRARY_PATH"
#export PATH="/soft/thetagpu/hpc-sdk/Linux_x86_64/21.3/comm_libs/mpi/bin:$PATH"
module restore default

# Global options
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
export OMP_NUM_THREADS=16

# This is GPUs
min_gpus=8
tpn=8

# Strong scaling.  Possibly not optimal due to requiring cubic meshblocks
if [[ $DO_STRONG == "true" ]]; then
  for size in 256 512 1024
  do
    gpus=$min_gpus
    while (( $gpus <= $N_NODES * 8 ))
    do
      np=$gpus

      nm=1
      div=1
      while (( $nm < $np ))
      do
          nm=$(( $nm * 8 ))
          div=$(( $div * 2 ))
      done
      msize=$(( $size / $div ))

      echo "cycle=100 Running $size cubed problem with KHARMA on $gpus gpus with $tpn tasks each (blocksize $msize)"

      mpirun -n $np -N $tpn --hostfile $COBALT_NODEFILE \
          $KHARMA_DIR/kharma.cuda "$@" parthenon/time/nlim=102 \
                      parthenon/mesh/nx1=$size parthenon/mesh/nx2=$size parthenon/mesh/nx3=$size \
                      parthenon/meshblock/nx1=$msize parthenon/meshblock/nx2=$msize parthenon/meshblock/nx3=$msize

      gpus=$(( $gpus * 2 ))
    done
  done
fi

# Weak scaling
if [[ $DO_WEAK == "true" ]]; then
  for size in 64 128
  do
    gpus=$min_gpus
    while (( $gpus <= $N_NODES * 8 ))
    do
      np=$gpus

      mul1=1
      mul2=1
      mul3=1
      if (( $np >= 2 )); then
          mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 4 )); then 
          mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 8 )); then 
          mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 16 )); then 
          mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 32 )); then 
          mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 64 )); then 
          mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 128 )); then 
          mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 256 )); then 
          mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 512 )); then
          mul1=$(( $mul1 * 2 ))
      fi
      tsize1=$(( $mul1 * $size ))
      tsize2=$(( $mul2 * $size ))
      tsize3=$(( $mul3 * $size ))
      nblock=$(( $mul1 * $mul2 * $mul3 ))
      echo "cycle=100 Running $size per node problem with KHARMA on $gpus gpus with $tpn tasks per node (total size ${tsize1}x${tsize2}x${tsize3}, $nblock blocks)"

      mpirun -n $np -N $tpn --hostfile $COBALT_NODEFILE \
          $KHARMA_DIR/kharma.cuda "$@" parthenon/time/nlim=102 \
                      parthenon/mesh/nx1=$tsize1 parthenon/mesh/nx2=$tsize2 parthenon/mesh/nx3=$tsize3 \
                      parthenon/meshblock/nx1=$size parthenon/meshblock/nx2=$size parthenon/meshblock/nx3=$size

      gpus=$(( $gpus * 2 ))
    done
  done
fi
