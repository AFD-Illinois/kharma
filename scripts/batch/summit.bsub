#!/bin/bash
#BSUB -P AST171
#BSUB -W 02:00
#BSUB -J KHARMA
# debug or batch
#BSUB -q batch

#BSUB -nnodes 16
NNODES=16

KHARMA_DIR=~/kharma

# Make bsub behave like sbatch, moving to CWD
cd $LS_SUBCWD

# Stuff for posterity
date
echo "Job run on nodes:"
jsrun -r $NNODES -r 1 hostname

# Just think of all the math we're going to have to do to automate scaling!
# -n # resource sets (== MPI tasks for us)
# -r # rs per node
# -a # MPI task per rs
# -g # GPU per rs
# -c # CPU cores (physical) per rs (total 42, but 7 is such an awkward number)
# -b binding strat, usually want packed every NCPU cores
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
export OMP_NUM_THREADS=28
jsrun --smpiargs="-gpu" -n $(($NNODES * 6)) -r 6 -a 1 -g 1 -c 7 -b packed:7 \
	$KHARMA_DIR/kharma.cuda -r torus.out1.00014.rhdf parthenon/output1/dt=30
