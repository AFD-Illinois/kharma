#!/bin/bash
#BSUB -P AST171
#BSUB -W 02:00
#BSUB -J KHARMA
# debug or batch
#BSUB -q debug

# ALWAYS check the following 3 lines before submitting the script

#BSUB -nnodes 1
NNODES=1
COMMAND="-i sane.par"

#==============

KHARMA_DIR=~/kharma

# Make bsub behave like sbatch, moving to CWD
# But don't cd anywhere under $HOME, that would be bad
if [[ "$LS_SUBCWD" !=  *"home"* ]]; then
  cd $LS_SUBCWD
fi

# Stuff for posterity
date
echo "Job run on nodes:"
jsrun -n $NNODES -r 1 hostname

# Tell OpenMP explicitly what it's dealing with
# This also ensures that if we set e.g. NUM_THREADS=6,
# they will be spread 1/core
# Remember to change this if using 7 cores!
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
export OMP_NUM_THREADS=24

# In order to run anything on the compute nodes, we must specify
# how we wish to slice them up.  This is done with arguments to
# 'jsrun', which takes the place of 'mpirun' on Summit
# -n # resource sets (== MPI tasks for us)
# -r # rs per node
# -a # MPI task per rs
# -g # GPU per rs
# -c # CPU cores (physical) per rs (total of 42=(22-1)*2, but 7 is such an awkward number)
# -b binding strat *within* a resource set

# The "smpiargs" argument is used to tell Spectrum MPI we're GPU-aware
jsrun --smpiargs="-gpu" -n $(($NNODES * 6)) -r 6 -a 1 -g 1 -c 6 -d packed -b packed:6 \
	$KHARMA_DIR/kharma.cuda $COMMAND
