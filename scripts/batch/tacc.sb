#!/bin/bash

# Submit/run script for KHARMA
# *should* work on any TACC system
#SBATCH -J kharma

# Appropriate queues:
# frontera: normal
# stampede2: flat-quadrant, normal
# longhorn: development, v100
#SBATCH -p normal

#SBATCH -N 32
#SBATCH --ntasks-per-node 1

#SBATCH -t 48:00:00
#SBATCH -o out-%j.txt

KHARMA_DIR=~/kharma

# Set tasks if SLURM doesn't
if [[ -z "$SLURM_NTASKS_PER_NODE" ]]; then
  export SLURM_NTASKS_PER_NODE=$(( $SLURM_NTASKS / $SLURM_NNODES ))
  export IBRUN_TASKS_PER_NODE=$(( $SLURM_NTASKS / $SLURM_NNODES ))
  echo "Assuming $SLURM_NTASKS_PER_NODE tasks per node"
fi

# OpenMP Options
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
# SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA
# This may one day not be true for Stampede2
echo "Splitting $SLURM_CPUS_ON_NODE cores among $SLURM_NTASKS_PER_NODE tasks"
export OMP_NUM_THREADS=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS_PER_NODE ))

# Limit Kokkos's view of the node if running on e.g. 2/4 GPUs for scaling
export KOKKOS_NUM_DEVICES=$SLURM_NTASKS_PER_NODE

# Profiling
#export KOKKOS_PROFILE_LIBRARY=$KHARMA_DIR/../kokkos-tools/kp_kernel_timer.so

# Machine-specific run lines
if [[ $(hostname -f) == *".stampede2.tacc.utexas.edu" || $(hostname -f) == *".frontera.tacc.utexas.edu" ]]; then
  ibrun tacc_affinity $KHARMA_DIR/kharma.host "$@"
elif [[ $(hostname -f) == *".longhorn.tacc.utexas.edu" ]]; then
  module unload python3 # Longhorn conda overrides MPI for some reason, kill it with fire
  export MY_SPECTRUM_OPTIONS="--gpu"
  ibrun $KHARMA_DIR/kharma.cuda "$@"
fi
