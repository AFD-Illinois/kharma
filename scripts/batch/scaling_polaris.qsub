#!/bin/bash

# Submit/run script for a KHARMA scaling test
# *should* work on Polaris

#PBS -N KHARMA
#PBS -l select=128
#PBS -l walltime=1:00:00
# large queues: prod, large, backfill-large
#PBS -q prod
#PBS -A gpu_hack
#PBS -l filesystems=home:grand
NNODES=`wc -l < $PBS_NODEFILE`
NRANKS=$(nvidia-smi -L | wc -l)

DO_STRONG=true
DO_WEAK=true

KHARMA_DIR=~/kharma-dev
WRAPPER=$KHARMA_DIR/bin/select_gpu_polaris

# Gotta specify this inline since bsub doesn't do arguments
PARFILE=~/kharma-dev/pars/benchmark/scaling_torus.par
# Allocate in full nodes, vs individual gpus
min_nodes=1
min_gpus=1 #$(( $NRANKS * $min_nodes ))

# OpenMP options
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
export OMP_NUM_THREADS=1

# MPI options
export MPICH_GPU_SUPPORT_ENABLED=1

# Profiling
#export KOKKOS_PROFILE_LIBRARY=$KHARMA_DIR/../kokkos-tools/kp_kernel_timer.so

# CD to working dir
cd $PBS_O_WORKDIR

# Stuff for posterity
date
echo "Job run on nodes:"
mpiexec --np $NNODES --ppn 1 hostname

# Strong scaling.  Possibly not optimal due to requiring cubic meshblocks
if [[ $DO_STRONG == "true" ]]; then
  for size in 192 384 768
  do
    gpus=$min_gpus
    while (( $gpus <= $NNODES * $NRANKS ))
    do
      np=$gpus

      nm=1
      div1=1
      div2=1
      div3=1
      to_div=3
      # Stop when we have half enough, since we'll use 2*N1 sized mesh
      while (( $nm < $np ))
      do
        nm=$(( $nm * 2 ))
        if [[ $to_div == "1" ]]; then
          div1=$(( $div1 * 2 ))
          to_div=3
        elif [[ $to_div == "2" ]]; then
          div2=$(( $div2 * 2 ))
          to_div=1
        else
          div3=$(( $div3 * 2 ))
          to_div=2
        fi
      done
      msize1=$(( $size / $div1 ))
      msize2=$(( $size / $div2 ))
      msize3=$(( $size / $div3 ))
 
      echo "cycle=100 Running ${size}x${size}x${size} cubed problem with KHARMA on $gpus GPUs (blocksize ${msize1}x${msize2}x${msize3})"

      mpiexec -n $gpus --ppn $NRANKS --depth 8 --cpu-bind depth --env OMP_NUM_THREADS=1 -env OMP_PLACES=threads $WRAPPER \
              $KHARMA_DIR/kharma.cuda -i $PARFILE parthenon/time/nlim=102 \
                                    parthenon/mesh/nx1=$size parthenon/mesh/nx2=$size parthenon/mesh/nx3=$size \
                                    parthenon/meshblock/nx1=$msize1 parthenon/meshblock/nx2=$msize2 parthenon/meshblock/nx3=$msize3

      gpus=$(( $gpus * 2 ))
    done
  done
fi

# Weak scaling
if [[ $DO_WEAK == "true" ]]; then
  for size in 64 128
  do
    gpus=$min_gpus
    while (( $gpus <= $NNODES * $NRANKS ))
    do
      np=$gpus

      # This runs the risk of wild inefficiencies
      # TODO find a decomposition that doesn't
      mul1=1
      mul2=1
      mul3=1
      if (( $np >= 2 )); then
        mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 4 )); then
        mul3=$(( $mul3 * 3 ))
      fi
      if (( $np >= 8 )); then 
        mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 16 )); then 
        mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 32 )); then 
        mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 64 )); then 
        mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 128 )); then 
        mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 256 )); then 
        mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 512 )); then 
        mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 1024 )); then
        mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 2048 )); then
        mul3=$(( $mul3 * 2 ))
      fi
      if (( $np >= 4096 )); then
        mul2=$(( $mul2 * 2 ))
      fi
      if (( $np >= 8192 )); then
        mul1=$(( $mul1 * 2 ))
      fi
      if (( $np >= 16384 )); then
        mul3=$(( $mul3 * 2 ))
      fi
      tsize1=$(( $mul1 * $size ))
      tsize2=$(( $mul2 * $size ))
      tsize3=$(( $mul3 * $size ))
      nblock=$(( $mul1 * $mul2 * $mul3 ))
      echo "cycle=100 Running $size per node problem with KHARMA on $gpus GPUs (total size ${tsize1}x${tsize2}x${tsize3}, $nblock blocks)"

      mpiexec -n $gpus --ppn $NRANKS --depth 8 --cpu-bind depth --env OMP_NUM_THREADS=1 -env OMP_PLACES=threads $WRAPPER \
            $KHARMA_DIR/kharma.cuda -i $PARFILE parthenon/time/nlim=102 \
                                    parthenon/mesh/nx1=$tsize1 parthenon/mesh/nx2=$tsize2 parthenon/mesh/nx3=$tsize3 \
                                    parthenon/meshblock/nx1=$size parthenon/meshblock/nx2=$size parthenon/meshblock/nx3=$size

      gpus=$(( $gpus * 2 ))
    done
  done
fi
