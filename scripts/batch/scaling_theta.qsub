#!/bin/bash

# These don't actually seem to do anything,
# but they document what to put on the command line
#COBALT -A NGM_EHT
#COBALT -t 60
#COBALT -n 4
# default, debug-flat-quad
#COBALT -q default
#COBALT --attrs mcdram=flat:numa=quad

KHARMA_DIR=~/kharma

# Act like sbatch
cd $QSUB_DIR

module restore default

echo "Starting Theta job script on " $(date)
echo "COBALT_PARTNAME: $COBALT_PARTNAME"
echo "COBALT_PARTSIZE: $COBALT_PARTSIZE"
echo "COBALT_JOBSIZE: $COBALT_JOBSIZE"
echo "COBALT_NODEFILE: $COBALT_NODEFILE"
echo "COBALT_JOBID: $COBALT_JOBID"
echo "Runs will use nodes:"
aprun -n $COBALT_JOBSIZE -N 1 hostname

DO_STRONG=true
DO_WEAK=true

KHARMA_DIR=~/kharma

# Global options
export OMP_PROC_BIND=spread
export OMP_PLACES=threads

n_hardware_threads_per_core=4
min_nodes=1

# Strong scaling.  Possibly not optimal due to requiring cubic meshblocks
if [[ $DO_STRONG == "true" ]]; then
  for size in 256 512 1024
  do
    for tpn in 1 2 4
    do
      # Use x/
      export OMP_NUM_THREADS=$(( 64 * $n_hardware_threads_per_core / $tpn ))

      nodes=$min_nodes
      while (( $nodes <= $COBALT_JOBSIZE ))
      do
        np=$(( $nodes * $tpn ))

        nm=1
        div=1
        while (( $nm < $np ))
        do
          nm=$(( $nm * 8 ))
          div=$(( $div * 2 ))
        done
        msize=$(( $size / $div ))

        echo "cycle=100 Running $size cubed problem with KHARMA on $nodes nodes with $tpn tasks each (blocksize $msize)"

        aprun -n $np -N $tpn \
          --env OMP_NUM_THREADS=$OMP_NUM_THREADS \
          -cc depth \
          -d $(( $OMP_NUM_THREADS / $n_hardware_threads_per_core * 4 )) \
          -j $n_hardware_threads_per_core \
          numactl -m 1 $KHARMA_DIR/kharma.host "$@" parthenon/time/nlim=102 \
                        parthenon/mesh/nx1=$size parthenon/mesh/nx2=$size parthenon/mesh/nx3=$size \
                        parthenon/meshblock/nx1=$msize parthenon/meshblock/nx2=$msize parthenon/meshblock/nx3=$msize

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi

# Weak scaling
if [[ $DO_WEAK == "true" ]]; then
  for size in 128
  do
    for tpn in 1 2 4
    do
      # SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA  
      # This may one day not be true for Stampede2
      export OMP_NUM_THREADS=$(( 64 * $n_hardware_threads_per_core / $tpn ))

      nodes=$min_nodes
      while (( $nodes <= $COBALT_JOBSIZE ))
      do
        np=$(( $nodes * $tpn ))

        mul1=1
        mul2=1
        mul3=1
        if (( $np >= 2 )); then
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 4 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 8 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 16 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 32 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 64 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 128 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 256 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 512 )); then
          mul1=$(( $mul1 * 2 ))
        fi
        tsize1=$(( $mul1 * $size ))
        tsize2=$(( $mul2 * $size ))
        tsize3=$(( $mul3 * $size ))
        nblock=$(( $mul1 * $mul2 * $mul3 ))
        echo "cycle=100 Running $size per node problem with KHARMA on $nodes nodes with $tpn tasks each (total size ${tsize1}x${tsize2}x${tsize3}, $nblock blocks)"

        aprun -n $np -N $tpn \
          --env OMP_NUM_THREADS=$OMP_NUM_THREADS \
          -cc depth \
          -d $(( $OMP_NUM_THREADS / $n_hardware_threads_per_core * 4 )) \
          -j $n_hardware_threads_per_core \
          numactl -m 1 $KHARMA_DIR/kharma.host "$@" parthenon/time/nlim=102 \
                        parthenon/mesh/nx1=$tsize1 parthenon/mesh/nx2=$tsize2 parthenon/mesh/nx3=$tsize3 \
                        parthenon/meshblock/nx1=$size parthenon/meshblock/nx2=$size parthenon/meshblock/nx3=$size

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi