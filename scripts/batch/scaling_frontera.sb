#!/bin/bash

# Submit/run script for a KHARMA scaling test
# *should* work on any SLURM-based system
#SBATCH -J kharma_scaling

# Appropriate queues:
# frontera: normal
# stampede2: flat-quadrant, normal
# longhorn: development, v100
#SBATCH -p normal

#SBATCH -N 64
#SBATCH --ntasks-per-node 2

#SBATCH -t 4:00:00
#SBATCH -o out-%j.txt

DO_STRONG=true
DO_WEAK=true

KHARMA_DIR=~/kharma

# Global options
export OMP_PROC_BIND=spread
export OMP_PLACES=threads

# Strong scaling.  Possibly not optimal due to requiring cubic meshblocks
if [[ $DO_STRONG == "true" ]]; then
  for size in 128 256
  do
    for tpn in 1 2
    do
      # Else for running in dev jobs
      export SLURM_NTASKS_PER_NODE=$tpn
      export IBRUN_TASKS_PER_NODE=$tpn
      # SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA
      # This may one day not be true for Stampede2
      export OMP_NUM_THREADS=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS_PER_NODE ))

      nodes=1
      while (( $nodes <= $SLURM_NNODES ))
      do
        np=$(( $nodes * $tpn ))

        nm=1
        div=1
        while (( $nm < $np ))
        do
          nm=$(( $nm * 8 ))
          div=$(( $div * 2 ))
        done
        msize=$(( $size / $div ))

        echo "cycle=100 Running $size cubed problem with KHARMA on $nodes nodes with $tpn tasks each (blocksize $msize)"

        ibrun -np $np tacc_affinity $KHARMA_DIR/kharma.host "$@" parthenon/time/nlim=102 \
                                    parthenon/mesh/nx1=$size parthenon/mesh/nx2=$size parthenon/mesh/nx3=$size \
                                    parthenon/meshblock/nx1=$msize parthenon/meshblock/nx2=$msize parthenon/meshblock/nx3=$msize

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi

# Weak scaling
if [[ $DO_WEAK == "true" ]]; then
  for size in 128
  do
    for tpn in 1 2
    do
      # Else for running in dev jobs
      export SLURM_NTASKS_PER_NODE=$tpn
      export IBRUN_TASKS_PER_NODE=$tpn
      # SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA  
      # This may one day not be true for Stampede2
      export OMP_NUM_THREADS=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS_PER_NODE ))

      nodes=1
      while (( $nodes <= $SLURM_NNODES ))
      do
        np=$(( $nodes * $tpn ))

        mul1=1
        mul2=1
        mul3=1
        if (( $np >= 2 )); then
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 4 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 8 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 16 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 32 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 64 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 128 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 256 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 512 )); then
          mul1=$(( $mul1 * 2 ))
        fi
        tsize1=$(( $mul1 * $size ))
        tsize2=$(( $mul2 * $size ))
        tsize3=$(( $mul3 * $size ))
        nblock=$(( $mul1 * $mul2 * $mul3 ))
        echo "cycle=100 Running $size per node problem with KHARMA on $nodes nodes with $tpn tasks each (total size ${tsize1}x${tsize2}x${tsize3}, $nblock blocks)"

        ibrun -np $np tacc_affinity $KHARMA_DIR/kharma.host "$@" parthenon/time/nlim=102 \
                                    parthenon/mesh/nx1=$tsize1 parthenon/mesh/nx2=$tsize2 parthenon/mesh/nx3=$tsize3 \
                                    parthenon/meshblock/nx1=$size parthenon/meshblock/nx2=$size parthenon/meshblock/nx3=$size

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi
