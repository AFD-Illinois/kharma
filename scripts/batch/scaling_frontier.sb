#!/bin/bash

# Submit/run script for a KHARMA scaling test on Delta
#SBATCH -J kharma_scaling
#SBATCH -A AST185
#SBATCH -p batch

#SBATCH -q debug
#SBATCH -t 2:00:00

#SBATCH -N 1
#SBATCH -o out-%j.txt

DO_STRONG=true
DO_WEAK=true

KHARMA_DIR=~/Code/kharma

# Strong scaling.  Possibly not optimal due to requiring cubic meshblocks
if [[ $DO_STRONG == "true" ]]; then
  for size in 512 1024
  do
    for tpn in 4
    do
      # Else for running in dev jobs
      export SLURM_NTASKS_PER_NODE=$tpn
      export IBRUN_TASKS_PER_NODE=$tpn
      # SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA
      # This may one day not be true for Stampede2
      export OMP_NUM_THREADS=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS_PER_NODE ))

      nodes=1
      while (( $nodes <= $SLURM_NNODES ))
      do
        np=$(( $nodes * $tpn ))

        nm=1
        div=1
        while (( $nm < $np ))
        do
          nm=$(( $nm * 8 ))
          div=$(( $div * 2 ))
        done
        msize=$(( $size / $div ))

        echo "cycle=100 Running $size cubed problem with KHARMA on $nodes nodes with $tpn tasks each (blocksize $msize)"

        srun -n $np $KHARMA_DIR/kharma.cuda -i $KHARMA_DIR/pars/scaling_torus.par parthenon/time/nlim=102 \
                                             parthenon/mesh/nx1=$size parthenon/mesh/nx2=$size parthenon/mesh/nx3=$size \
                                             parthenon/meshblock/nx1=32 parthenon/meshblock/nx2=32 parthenon/meshblock/nx3=32

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi

# Weak scaling
if [[ $DO_WEAK == "true" ]]; then
  for size in 64 128
  do
    for tpn in 4
    do
      # Else for running in dev jobs
      export SLURM_NTASKS_PER_NODE=$tpn
      export IBRUN_TASKS_PER_NODE=$tpn
      # SLURM schedules us on all CPUs, which is nearly always what we want with KHARMA  
      # This may one day not be true for Stampede2
      export OMP_NUM_THREADS=$(( $SLURM_CPUS_ON_NODE / $SLURM_NTASKS_PER_NODE ))

      nodes=1
      while (( $nodes <= $SLURM_NNODES ))
      do
        np=$(( $nodes * $tpn ))

        mul1=1
        mul2=1
        mul3=1
        if (( $np >= 2 )); then
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 4 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 8 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 16 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 32 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 64 )); then 
          mul1=$(( $mul1 * 2 ))
        fi
        if (( $np >= 128 )); then 
          mul3=$(( $mul3 * 2 ))
        fi
        if (( $np >= 256 )); then 
          mul2=$(( $mul2 * 2 ))
        fi
        if (( $np >= 512 )); then
          mul1=$(( $mul1 * 2 ))
        fi
        tsize1=$(( $mul1 * $size ))
        tsize2=$(( $mul2 * $size ))
        tsize3=$(( $mul3 * $size ))
        nblock=$(( $mul1 * $mul2 * $mul3 ))
        echo "cycle=100 Running $size per node problem with KHARMA on $nodes nodes with $tpn tasks each (total size ${tsize1}x${tsize2}x${tsize3}, $nblock blocks)"

        srun -n $np $KHARMA_DIR/kharma.cuda -i $KHARMA_DIR/pars/scaling_torus.par parthenon/time/nlim=102 \
                                             parthenon/mesh/nx1=$tsize1 parthenon/mesh/nx2=$tsize2 parthenon/mesh/nx3=$tsize3 \
                                             parthenon/meshblock/nx1=$size parthenon/meshblock/nx2=$size parthenon/meshblock/nx3=$size

        nodes=$(( $nodes * 2 ))

      done
    done
  done
fi
