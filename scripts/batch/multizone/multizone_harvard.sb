#!/bin/bash
# Admin stuff
#SBATCH -J KHARMA-MZ
#SBATCH -t 160:00:00
#SBATCH -N 1
##SBATCH -n 4
#SBATCH --gres=gpu:1 #4
#SBATCH -o "out-%j.txt"

# Partition
#SBATCH -p itc_gpu

####SBATCH --tasks-per-node=4
####SBATCH --cpus-per-task=16
####SBATCH --exclusive
####SBATCH --mem=0
#SBATCH --mem=8G

PROB=bflux
RES=32
DIM=2 #3
NZONES=4 #8
BASE=8
NRUNS=3000

args=()

# meshblocks
args+=(" --nx1=$RES --nx2=$RES --nx1_mb=$RES")
if [[ $DIM -gt 2 ]]; then
  args+=(" --nx2_mb=$(($RES/2)) --nx3=$RES --nx3_mb=$(($RES/2))")
else
  args+=(" --nx2_mb=$RES --nx3=1 --nx3_mb=1")
fi

# common things
args+=(" --nzones=$NZONES --base=$BASE --nruns=$NRUNS --spin=0.0 --nlim=$((100000*$RES*$RES/64/64))")

if [[ "$PROB" == *"bflux"* ]]; then
  if [[ $NZONES -eq 8 ]]; then
    bz=2e-8
  elif [[ $NZONES -eq 4 ]]; then
    bz=1e-8
  elif [[ $NZONES -eq 3 ]]; then
    bz=1e-4
  else
    bz=2e-8
  fi
  args+=(" --bz=$bz --kharma_bin=../kharma_faster_rst_fixed.cuda")
fi

source ~/venv3/bin/activate
KHARMA_DIR=/n/holylfs05/LABS/bhi/Users/hyerincho/grmhd/kharma_fork
#HOST=$(hostname -f)
#ARGS="clean trace cuda gcc10"
#for machine in $KHARMA_DIR/machines/*.sh
#do
#  source $machine
#done

#export OMP_PROC_BIND=spread
#export OMP_PLACES=threads
#KOKKOS_MAP_DEVICE_ID_BY=mpi_rank

# Everything is called from the supervising python script
# No point in setting a walltime limit, this invokes KHARMA many times
exec $KHARMA_DIR/scripts/batch/multizone/run.py ${args[@]}
